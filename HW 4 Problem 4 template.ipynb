{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782b1aeb-44a8-4d40-b2e5-05929ff2116e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook we will implement value iteration and use it to solve the gambler's problem. This is based on the book by Sutton-Barto, specifically this version: https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbb63d-ff97-4a0d-91f3-105605af48bb",
   "metadata": {},
   "source": [
    "### Choosing a $\\gamma$\n",
    "Recall that $\\gamma \\in [0, 1]$ is the discount factor used to compute the expected discounted returns\n",
    "$$G_t = \\sum_{k = 0}^\\infty \\gamma^k R_{t + k + 1}.$$\n",
    "This hyperparameter $\\gamma$ controls how farsighted the agent is, and it also ensures that $G_t$ is finite in the case of infinitely many bounded nonzero terms. (See Sections 3.3, 3.4 in Sutton-Barto.) Choose $\\gamma$ for the present case of a finite MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283afba-ea07-48ef-bec5-b4d09f87a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define `GAMMA`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9dccf-3836-4cca-af91-974c3ed7a529",
   "metadata": {},
   "source": [
    "### General objects\n",
    "Here is a general class template that represents a finite Markov decision process (MDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa0eb7-f162-4a4f-ac30-f8bd0c25e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MDP:\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The states of this MDP.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def actions(self, state):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The actions allowed from `state`.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def psr(self, state, action):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The set of tuples (p, s, r) where `s` is a state reachable\n",
    "                from `state` by performing `action`, `p` is the probability\n",
    "                of reaching `s`, and `r` is the reward gained by reaching `s`.\n",
    "                In particular, the sum of the `p` equals 1.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58af957-b50d-458e-ac2f-453c4b9d1806",
   "metadata": {},
   "source": [
    "### Implementing the gambler's problem\n",
    "To make our implementation compatible with value iteration, we will include the state `self.goal`. Recall that the reward of reaching the goal is $1$ and that the reward of reaching any other state is $0.$ Moreover, recall that the MDP ends once either `0` or `self.goal` is reached. How should the state `self.goal` behave, e.g. what should the available actions and rewards from this state be? With this in mind, fix the below naive implementation of the gambler's problem. You can use the rest of the notebook (in particular the last cell) to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a36d49-8a64-4bcb-9d82-ad855e3aeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamblersProblem(MDP):\n",
    "    '''\n",
    "    The gambler's problem, as described in Example 4.3 of Sutton-Barto.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, p_h, goal):\n",
    "        assert 0 <= p_h <= 1\n",
    "        self.p_h = p_h\n",
    "        self.goal = goal\n",
    "\n",
    "    @property\n",
    "    def states(self): # TODO: fix this\n",
    "        return set(range(self.goal))\n",
    "\n",
    "    def actions(self, state): # TODO: fix this\n",
    "        return set(range(min(state, self.goal - state)))\n",
    "\n",
    "    def psr(self, state, action): # TODO: fix this\n",
    "        return {\n",
    "            (\n",
    "                self.p_h, # heads, i.e. the gambler wins money\n",
    "                state + action,\n",
    "                1 if state + action == self.goal else 0\n",
    "            ), (\n",
    "                1 - self.p_h, # tails, i.e. the gambler loses money\n",
    "                state - action,\n",
    "                0\n",
    "            ) \n",
    "        }\n",
    "\n",
    "    def plot_policy(self, policy):\n",
    "        for s, actions in policy.items():\n",
    "            xy = [(s, action) for action in actions]\n",
    "            plt.scatter(\n",
    "                *zip(*xy),\n",
    "                # purple if there are multiple actions\n",
    "                color='purple' if len(actions) > 1 else 'blue', \n",
    "                marker='.'\n",
    "            )\n",
    "\n",
    "    def plot_value(self, value):\n",
    "        xy = [(s, v) for s, v in value.items()]\n",
    "        plt.plot(*zip(*xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31535dba-2723-4a5e-8811-2aeb52e64eb3",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "\n",
    "Here is a general class that represents a value function on a finite MDP. Implement the `truncated_evaluations` and `greedy_policy` methods, replacing the currently obfuscated code, recalling that the truncated evaluation (see Section 4.4 in Sutton-Barto) for a value function $v,$ a state $s,$ and an action $a$ is\n",
    "$$\\mathbb{E}[R_{t + 1} + \\gamma v(S_{t + 1}) \\mid S_t = s, A_t = a] = \\sum_{(s', r)} p(s', r \\mid s, a)[r + \\gamma v(s')].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc77dca-4b5e-4e9a-9d2a-3e8eb26fd68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(dict):\n",
    "\n",
    "    def truncated_evaluations(self, mdp): # TODO: implement this\n",
    "        '''\n",
    "        Returns:\n",
    "            (dict) The truncated policy evaluations for every state and\n",
    "                every action. The keys are the states, and the values are\n",
    "                dicts keyed by actions with value the truncated policy\n",
    "                evaluation of performing the action at the state.\n",
    "        '''\n",
    "        # the following code works (so that the rest of the notebook works)\n",
    "        # but is obfuscated\n",
    "        dontlookatthis = {}\n",
    "        for a in mdp.states:\n",
    "            stoplooking = {}\n",
    "            for imaginegradingthis in mdp.actions(a):\n",
    "                painfulamirite = 0\n",
    "                for s_prime, p, s in mdp.psr(a, imaginegradingthis):\n",
    "                    painfulamirite += s_prime * (s + GAMMA * self[p])\n",
    "                stoplooking[imaginegradingthis] = painfulamirite\n",
    "            dontlookatthis[a] = stoplooking\n",
    "        return dontlookatthis\n",
    "\n",
    "    def greedy_policy(self, mdp, tiebreak='equal_split'):\n",
    "        '''\n",
    "        The greedy policy (with respect to the truncated policy evaluations).\n",
    "\n",
    "        Args:\n",
    "            tiebreaker: Decides what to do in case there are multiply actions\n",
    "                with the same truncated policy evaluation. The options are\n",
    "                'equal_split' (of the probabilities), 'max' (of the actions),\n",
    "                'min' (of the actions), 'random' (action).\n",
    "        '''\n",
    "        greedy_actions = {\n",
    "            s: {a for a, v in evals.items() if v == max(evals.values())}\n",
    "            for s, evals in self.truncated_evaluations(mdp).items()\n",
    "        }\n",
    "        tiebreaker = lambda actions: {\n",
    "            'equal_split': {a: 1 / len(actions) for a in actions},\n",
    "            'max': {max(actions): 1.0},\n",
    "            'min': {min(actions): 1.0},\n",
    "            'random': {random.choice(list(actions)): 1.0},\n",
    "        }[tiebreak]\n",
    "        return {\n",
    "            s: tiebreaker(actions)\n",
    "            for s, actions in greedy_actions.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc89902-8c3b-40e7-b1b2-518aa1a3eae0",
   "metadata": {},
   "source": [
    "### Value iteration algorithm\n",
    "The pseudocode for value iteration in Sutton-Barto (see Figure 4.5 in Sutton-Barto on page 101) has a slight error. Namely, it is using asynchronous dynamic programming (see the following Section 4.5 in Sutton-Barto) because it updates $V(s)$ in place, i.e. it is using the updated value of $V(s)$ for the subsequent computations of $V(s')$ for other $s'.$ The pseudocode is also non-Pythonic. Below is an implementation following their pseudocode, which we will use during section. Reimplement this so that it is the intended algorithm and in a Pythonic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f32df-6b77-494b-a83e-f2e222b79b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_algorithm(mdp, theta=1e-15, max_iter=200, tiebreak='equal_split'):\n",
    "    '''\n",
    "    The value iteration algorithm (but not really, because see above) following\n",
    "    the pseudocode in Sutton-Barto.\n",
    "    '''\n",
    "    # TODO: Reimplement this.\n",
    "    \n",
    "    V = Value({state: 0 for state in mdp.states})\n",
    "    pbar = tqdm(range(max_iter))\n",
    "    for iter in pbar:\n",
    "        Delta = 0\n",
    "        for s in mdp.states:\n",
    "            v = V[s]\n",
    "            V[s] = max(V.truncated_evaluations(mdp)[s].values())\n",
    "            Delta = max(Delta, abs(v - V[s]))\n",
    "        if Delta < theta:\n",
    "            break\n",
    "            \n",
    "        mdp.plot_value(V)\n",
    "        pbar.set_description(f'{Delta=:<6.3}  {V[50]=:<6.3}  {V[100]=:<6.3}')\n",
    "    plt.show()\n",
    "    \n",
    "    return V.greedy_policy(mdp, tiebreak=tiebreak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab25fc3-9b55-4fc8-93ad-7c1e3f96c4f3",
   "metadata": {},
   "source": [
    "### Playing around with the gambler's problem\n",
    "Use the cell below to test various values of $p_h.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd4d96-23dd-492e-9987-2839fcc40821",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GamblersProblem(0.40, 100)\n",
    "mdp.plot_policy(value_iteration_algorithm(mdp, theta=1e-15, tiebreak='equal_split'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3579b-8677-4942-bfa9-deadc865e64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
