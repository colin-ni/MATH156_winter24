{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782b1aeb-44a8-4d40-b2e5-05929ff2116e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook we will implement value iteration and use it to solve the gambler's problem and GridWorld. This is based on the book by Sutton-Barto, specifically this version: https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbb63d-ff97-4a0d-91f3-105605af48bb",
   "metadata": {},
   "source": [
    "### Choosing a $\\gamma$\n",
    "Recall that $\\gamma \\in [0, 1]$ is the discount factor used to compute the expected discounted returns\n",
    "$$G_t = \\sum_{k = 0}^\\infty \\gamma^k R_{t + k + 1}.$$\n",
    "This hyperparameter $\\gamma$ controls how farsighted the agent is, and it also ensures that $G_t$ is finite in the case of infinitely many bounded nonzero terms. (See Sections 3.3, 3.4 in Sutton-Barto.) Choose $\\gamma$ for the present case of a finite MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848db962-eca6-46e6-ae90-bd3993041d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9dccf-3836-4cca-af91-974c3ed7a529",
   "metadata": {},
   "source": [
    "### General objects\n",
    "Here is a general class template that represents a finite Markov decision process (MDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa0eb7-f162-4a4f-ac30-f8bd0c25e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MDP:\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The states of this MDP.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def actions(self, state):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The actions allowed from `state`.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def psr(self, state, action):\n",
    "        '''\n",
    "        Returns:\n",
    "            (set) The set of tuples (p, s, r) where `s` is a state reachable\n",
    "                from `state` by performing `action`, `p` is the probability\n",
    "                of reaching `s`, and `r` is the reward gained by reaching `s`.\n",
    "                In particular, the sum of the `p` equals 1.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def sink_states(self):\n",
    "        return {\n",
    "            state\n",
    "            for state in self.states\n",
    "            if all(\n",
    "                psr[1] == state # every psr goes back to state\n",
    "                for action in self.actions(state)\n",
    "                for psr in self.psr(state, action)\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def approximate_value(self, policy, state, n_samples=int(1e3)):\n",
    "        if n_samples == 1:\n",
    "            curr_state = state\n",
    "            total_reward = 0\n",
    "            while curr_state not in self.sink_states:\n",
    "                # choose an action randomly according to the policy\n",
    "                actions, probs = list(zip(*policy[curr_state].items()))\n",
    "                action = random.choices(actions, weights=probs, k=1)[0]\n",
    "                # choose a psr randomly according to the MDP\n",
    "                p_list, s_list, r_list = list(zip(*self.psr(curr_state, action)))\n",
    "                s, r = random.choices(list(zip(s_list, r_list)), weights=p_list, k=1)[0]\n",
    "                total_reward += r\n",
    "                curr_state = s  \n",
    "            return total_reward\n",
    "        else:\n",
    "            return sum(\n",
    "                self.approximate_value(policy, state, n_samples=1)\n",
    "                for _ in tqdm(range(n_samples))\n",
    "            ) / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58af957-b50d-458e-ac2f-453c4b9d1806",
   "metadata": {},
   "source": [
    "### Implementing the gambler's problem\n",
    "To make our implementation compatible with value iteration, we will include the state `self.goal`. Recall that the reward of reaching the goal is $1$ and that the reward of reaching any other state is $0.$ Moreover, recall that the MDP ends once either `0` or `self.goal` is reached. How should the state `self.goal` behave, e.g. what should the available actions and rewards from this state be? Complete the below implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a36d49-8a64-4bcb-9d82-ad855e3aeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamblersProblem(MDP):\n",
    "    '''\n",
    "    The gambler's problem, as described in Example 4.3 of Sutton-Barto.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, p_h, goal):\n",
    "        assert 0 <= p_h <= 1\n",
    "        self.p_h = p_h\n",
    "        self.goal = goal\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return set(range(self.goal + 1))\n",
    "\n",
    "    def actions(self, state):\n",
    "        if state in {0, self.goal}:\n",
    "            # don't allow any actions once the goal is reached, since otherwise\n",
    "            # one could reach the goal multiple times and gain repeated rewards\n",
    "            return {0}\n",
    "        else:\n",
    "            # don't allow bets of zero\n",
    "            return set(range(1, min(state, self.goal - state) + 1))\n",
    "\n",
    "    def psr(self, state, action):\n",
    "        return {\n",
    "            (\n",
    "                self.p_h, # heads, i.e. the gambler wins money\n",
    "                state + action,\n",
    "                1 if state + action == self.goal != state else 0\n",
    "            ), (\n",
    "                1 - self.p_h, # tails, i.e. the gambler loses money\n",
    "                state - action,\n",
    "                0\n",
    "            ) \n",
    "        }\n",
    "\n",
    "    def plot_policy(self, policy):\n",
    "        for s, actions in policy.items():\n",
    "            xy = [(s, action) for action in actions]\n",
    "            plt.scatter(\n",
    "                *zip(*xy),\n",
    "                # purple if there are multiple actions\n",
    "                color='purple' if len(actions) > 1 else 'blue', \n",
    "                marker='.'\n",
    "            )\n",
    "\n",
    "    def plot_value(self, value):\n",
    "        xy = [(s, v) for s, v in value.items()]\n",
    "        plt.plot(*zip(*xy))\n",
    "\n",
    "\n",
    "assert GamblersProblem(0.45, 100).sink_states == {0, 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31535dba-2723-4a5e-8811-2aeb52e64eb3",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "\n",
    "Here is a general class that represents a value function on a finite MDP. Implement the `truncated_evaluations` method, recalling that the truncated evaluation (see Section 4.4 in Sutton-Barto) for a value function $v,$ a state $s,$ and an action $a$ is\n",
    "$$\\mathbb{E}[R_{t + 1} + \\gamma v(S_{t + 1}) \\mid S_t = s, A_t = a] = \\sum_{(s', r)} p(s', r \\mid s, a)[r + \\gamma v(s')].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc77dca-4b5e-4e9a-9d2a-3e8eb26fd68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(dict):\n",
    "\n",
    "    def truncated_evaluations(self, mdp):\n",
    "        '''\n",
    "        Returns:\n",
    "            (dict) The truncated policy evaluations for every state and\n",
    "                every action. The keys are the states, and the values are\n",
    "                dicts keyed by actions with value the truncated policy\n",
    "                evaluation of performing the action at the state.\n",
    "        '''\n",
    "        return {\n",
    "            s: {\n",
    "                a : sum(\n",
    "                    p * (r + GAMMA * self[s_prime])\n",
    "                    for p, s_prime, r in mdp.psr(s, a)\n",
    "                )\n",
    "                for a in mdp.actions(s)\n",
    "            }\n",
    "            for s in mdp.states\n",
    "        }\n",
    "\n",
    "    def greedy_policy(self, mdp, tiebreak='equal_split'):\n",
    "        '''\n",
    "        The greedy policy (with respect to the truncated policy evaluations).\n",
    "\n",
    "        Args:\n",
    "            tiebreaker: Decides what to do in case there are multiply actions\n",
    "                with the same truncated policy evaluation. The options are\n",
    "                'equal_split' (of the probabilities), 'max' (of the actions),\n",
    "                'min' (of the actions), 'random' (action).\n",
    "        '''\n",
    "        greedy_actions = {\n",
    "            s: {a for a, v in evals.items() if v == max(evals.values())}\n",
    "            for s, evals in self.truncated_evaluations(mdp).items()\n",
    "        }\n",
    "        tiebreaker = lambda actions: {\n",
    "            'equal_split': {a: 1 / len(actions) for a in actions},\n",
    "            'max': {max(actions): 1.0},\n",
    "            'min': {min(actions): 1.0},\n",
    "            'random': {random.choice(list(actions)): 1.0},\n",
    "        }[tiebreak]\n",
    "        return {\n",
    "            s: tiebreaker(actions)\n",
    "            for s, actions in greedy_actions.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc89902-8c3b-40e7-b1b2-518aa1a3eae0",
   "metadata": {},
   "source": [
    "### Value iteration algorithm\n",
    "Implement the value iteration algorithm in a Pythonic way. (See Figure 4.5 in Sutton-Barto on page 101, but implement it in a sane way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f32df-6b77-494b-a83e-f2e222b79b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_algorithm(mdp, theta=1e-15, max_iter=2000, tiebreak='equal_split'):\n",
    "    '''\n",
    "    The value iteration algorithm. \n",
    "    '''\n",
    "    v = Value({state: 0 for state in mdp.states})\n",
    "    pbar = tqdm(range(max_iter))\n",
    "    for iter in pbar:\n",
    "        tpe = v.truncated_evaluations(mdp)\n",
    "        new_v = Value({s: max(tpe[s].values()) for s in mdp.states})\n",
    "        Delta = max(abs(v[s] - new_v[s]) for s in mdp.states)\n",
    "        v = new_v\n",
    "        if float(Delta) < theta:\n",
    "            print(f'{Delta=:.3} reached threshold {theta=:.3}')\n",
    "            break\n",
    "        mdp.plot_value(v)\n",
    "        pbar.set_description(f'{Delta=:.3}')\n",
    "    plt.show()\n",
    "    return v.greedy_policy(mdp, tiebreak=tiebreak), v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab25fc3-9b55-4fc8-93ad-7c1e3f96c4f3",
   "metadata": {},
   "source": [
    "### Playing around with the gambler's problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c7fb1-b867-4985-ba3a-d846c73c57e2",
   "metadata": {},
   "source": [
    "We will use a goal of $32$ instead of $100$ because it is faster to compute. For $p_h = 0.25,$ value iteration converges within 10 iterations and produces a nontrivial policy involving betting everything at the state $16.$ We can approximate the value of this strategy in two ways: using the truncated evaluations of the policy and using monte carlo simulations (`MDP.approximate_value`). The truncated evaluation is exactly $0.25$ whereas the monte carlo simulation is approximately $0.25$ (this is just sampling a Bernoulli random variable), corresponding to the probability of winning the coin flip at the state $16.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb102148-3d4d-45f6-867c-c832f05d0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h = 0.25\n",
    "mdp = GamblersProblem(p_h, 32)\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-2, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy)\n",
    "plt.show()\n",
    "\n",
    "print(f'truncated evaluation value is {value[16]}')\n",
    "print(f'monte carlo approximated value is {mdp.approximate_value(policy, 16, n_samples=int(1e4))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20851618-5870-4683-a3da-526b8cfd461b",
   "metadata": {},
   "source": [
    "For $p_h = 0.55,$ consider stopping the value iteration early so that it produces a nontrivial policy involving bets of greater than 1 dollar. The value of this non-optimal policy is significantly lower than the optimal policy of always betting exactly 1 dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd4d96-23dd-492e-9987-2839fcc40821",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h = 0.55\n",
    "mdp = GamblersProblem(p_h, 32)\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-2, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy)\n",
    "plt.show()\n",
    "\n",
    "print(f'monte carlo approximated value is {mdp.approximate_value(policy, 16, n_samples=int(1e3)):.3}')\n",
    "print(f'truncated evaluation value is {value[16]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf62d30-8a8d-47e8-84c6-dba1ba863403",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h = 0.55\n",
    "mdp = GamblersProblem(p_h, 32)\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-3, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy)\n",
    "plt.show()\n",
    "\n",
    "print(f'monte carlo approximated value is {mdp.approximate_value(policy, 16, n_samples=int(200))}')\n",
    "print(f'truncated evaluation value is {value[16]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b44f2-dac9-454a-bc36-5ce5e633051f",
   "metadata": {},
   "source": [
    "### GridWorld\n",
    "Here is an implementation of GridWorld, introduced in Example 3.8 on page 72 of https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf. Because this MDP lasts forever for reasonable policies, we will choose `GAMMA = 0.9` to get finite values. To do this, change `GAMMA` at the beginning of the notebook, and rerun all cells above this one (lmao)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67e679-e170-4c31-8fd5-12323b93ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "assert GAMMA < 1, 'Set GAMMA to 0.9 at the top of the notebook, then rerun all cells. (See text above.)'\n",
    "\n",
    "class GridWorld(MDP):\n",
    "    def __init__(self, n, portals):\n",
    "        self.n = n\n",
    "        self.portals = portals\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return {(i, j) for i in range(self.n) for j in range(self.n)}\n",
    "\n",
    "    def actions(self, state):\n",
    "        return {(-1, 0), (0, 1), (1, 0), (0, -1)} # NESW\n",
    "\n",
    "    def in_grid(self, i, j):\n",
    "        return 0 <= i < self.n and 0 <= j < self.n\n",
    "\n",
    "    def psr(self, state, action):\n",
    "        # note there is only one psr because the result of an action is always determinisitic\n",
    "        p = 1.0\n",
    "        \n",
    "        if state in self.portals:\n",
    "            s, r = self.portals[state] # travel through a portal and get the reward for it\n",
    "        else:\n",
    "            i, j = state\n",
    "            di, dj = action\n",
    "            if self.in_grid(i + di, j + dj):\n",
    "                s = (i + di, j + dj) # simply move according to the action\n",
    "                r = 0.0\n",
    "            else:\n",
    "                s = state # don't move\n",
    "                r = -1.0 # get penalized by 1.0 for trying to move out of the grid\n",
    "        return {(p, s, r)}\n",
    "\n",
    "    def plot_value(self, value):\n",
    "        '''Don't plot value functions'''\n",
    "        pass\n",
    "\n",
    "    def plot_policy(self, policy, value=None):\n",
    "        '''It works, so don't worry about it.'''\n",
    "        ij_to_plt = lambda i, j: (j, self.n - i)\n",
    "        arrow = {(0, 1) : (1, 0), (0, -1) : (-1, 0), (-1, 0) : (0, 1), (1, 0) : (0, -1)}\n",
    "        fig, ax = plt.subplots(figsize=(self.n + 1, self.n + 1))\n",
    "        plt.axis('off')\n",
    "        for (i, j), actions in policy.items():\n",
    "            for action in actions:\n",
    "                plt.arrow(\n",
    "                    j, self.n - i,\n",
    "                    0.2 * arrow[action][0], 0.2 * arrow[action][1],\n",
    "                    head_width=0.05\n",
    "                )\n",
    "        colors = plt.get_cmap('spring')(np.linspace(0, 1, len(self.portals)))\n",
    "        for (portal_start, (portal_end, reward)), color in zip(self.portals.items(), colors):\n",
    "            plt.scatter(*ij_to_plt(*portal_start), s=100, color=color)\n",
    "            plt.scatter(*ij_to_plt(*portal_end), s=400, color=color)\n",
    "            plt.text(*ij_to_plt(*portal_start), str(reward), horizontalalignment='left', verticalalignment='bottom', size=20)\n",
    "        if value is not None:\n",
    "            for state, v in value.items():\n",
    "                plt.text(*ij_to_plt(*state), str(round(v, 1)), horizontalalignment='right', verticalalignment='top', size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c58bb9-7f14-43d7-82b5-314de956f5ef",
   "metadata": {},
   "source": [
    "We can easily solve the example given in lecture and in the book. Here is how to interpret the plots:\n",
    "- colored disks represent the portals, where the small ones are the starts and the big ones are the destinations\n",
    "- large numbers indicate the reward for entering the portal (i.e. being at the portal start and taking the unique action to the portal destination)\n",
    "- arrows represent the equally-likely actions of an optimal policy\n",
    "- small numbers are the values of that optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812b652-eeaa-4bd2-b08e-7f11e2b735ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridWorld(5, {(0, 1): ((4, 1), 10), (0, 3): ((2, 3), 5)})\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-5, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c94ce-2747-4e4d-bc2f-ab910ca1efbe",
   "metadata": {},
   "source": [
    "Consider a one-way torus, and fix a point $p$ on it. What is the shortest path to $p$? Simply follow the arrows in the folliwing policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8731b3-868d-41b4-af3e-3866aa6c7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "p_i, p_j = (4, 3)\n",
    "\n",
    "portals_from_right_to_left = {(i, n - 1): ((i, 0), 0) for i in range(n)}\n",
    "portals_from_bottom_to_top = {(n - 1, i): ((0, i), 0) for i in range(n - 1)}\n",
    "destination = {(p_i, p_j): ((p_i, p_j), 1)}\n",
    "mdp = GridWorld(\n",
    "    n,\n",
    "    {**portals_from_right_to_left, **portals_from_bottom_to_top, **destination}\n",
    ")\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-5, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb095a-781a-466f-98ef-24d3628713b8",
   "metadata": {},
   "source": [
    "How did Walter White decide to pursue a life of crime? He took into account his life expectancy (`GAMMA`) and the expected pay. In the following cell, try adjusting the various pay levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001cc73-7649-442d-84fd-77467e4094f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "normal_pay = 1\n",
    "small_crime_pay = 4\n",
    "big_crime_pay = 16\n",
    "\n",
    "small_crime = { # pay is a little better than usual\n",
    "    (4, 13): ((11, 12), small_crime_pay),\n",
    "    (3, 12): ((3, 13), normal_pay),\n",
    "    (3, 14): ((4, 14), normal_pay),\n",
    "    (5, 14): ((5, 13), normal_pay),\n",
    "    (5, 12): ((4, 12), normal_pay),\n",
    "}\n",
    "\n",
    "big_crime = { # pay is way better than usual\n",
    "    (11, 4): ((12, 11), big_crime_pay),\n",
    "    (10, 3): ((10, 4), normal_pay),\n",
    "    (10, 5): ((11, 5), normal_pay),\n",
    "    (12, 5): ((12, 4), normal_pay),\n",
    "    (12, 3): ((11, 3), normal_pay),\n",
    "}\n",
    "\n",
    "\n",
    "jail_bounds = { # you cannot leave\n",
    "    (i, j): ((i, j), -100) \n",
    "    for i in range(n)\n",
    "    for j in range(n)\n",
    "    if 10 <= i <= 13 and 10 <= j <= 13 and (i in {10, 13} or j in {10, 13})\n",
    "}\n",
    "\n",
    "mdp = GridWorld(\n",
    "    n,\n",
    "    {**small_crime, **big_crime, **jail_bounds}\n",
    ")\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-5, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcef52-0bbc-4183-b16a-d9d25cd2d681",
   "metadata": {},
   "source": [
    "You can create your own grid-world here, if you are feeling creative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc57c2-d749-44a7-8a29-3c78f2eaf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "portals = {\n",
    "    # start_location: (end_location, reward)\n",
    "    (1, 5): ((13, 8), 10)\n",
    "}\n",
    "mdp = GridWorld(n, portals)\n",
    "policy, value = value_iteration_algorithm(mdp, theta=1e-5, tiebreak='equal_split')\n",
    "mdp.plot_policy(policy, value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc83cc-8675-43b8-b4c8-33a8564cb384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
